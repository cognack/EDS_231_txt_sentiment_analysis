---
title: "03_sentiment_analysis_homework"
author: "Steven Cognac"
date: "4/13/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyr) #text analysis in R
library(lubridate) #working with date data
# library(pdftools) #read in pdfs
library(tidyverse)
library(tidytext)
library(here)
library(LexisNexisTools) #Nexis Uni data wrangling
library(sentimentr)
library(readr)
# library(textdata)
```



# Import files
 - keyword search 'inaturalist'
 - (iNaturalist)[https://www.inaturalist.org/] is a nature app that helps you identify the plants and animals. 
 
```{r, message=FALSE}
my_files <- list.files(pattern = ".docx",
                       path = here(),
                       full.names = TRUE,
                       recursive = TRUE, 
                       ignore.case = TRUE)

# Object of class 'LNT output'
dat <- lnt_read(my_files)
```

## Let's look at some of the data of our articles.
```{r}
# isolating matedata, articles, and paragraphs
meta_df <- dat@meta
articles_df <- dat@articles
paragraphs_df <- dat@paragraphs

# put the meta_df, articles_df, and paragraphs_df into a dataframe
dat2 <- tibble(element_id = seq(1:length(meta_df$Headline)),
                  Date = meta_df$Date, 
                  Headline = meta_df$Headline)

# May be of use for assignment: using the full text from the articles
paragraphs_dat <- data_frame(element_id = paragraphs_df$Art_ID,
                             Text  = paragraphs_df$Paragraph)

# join dat2 with paragraphs_df
dat3 <- inner_join(dat2, paragraphs_dat, by = "element_id")
dat3
```

# Cleaning the Data
Now lets clean up dat3 in the code chunks below. First we need to unnest the `text` column to the word level so we can extract the individual words. Next we'll remove unwanted character patterns. 


We want to remove unwanted data in the `Text` column as these are not actual paragraphs. Patterns we'll remove include:

 1. Web address pattern "[ 1]: http..."
 2. Paragraphs less than 40 characters

```{r clean_data}
# remove paragraphs that contain website links,  or are shorter than 21 characters
dat4 <- dat3 %>% 
  mutate(link = str_detect(dat3$Text, "1]: http://", negate = FALSE),
         txt_short = nchar(dat3$Text) < 40) %>%
  filter(link == FALSE & txt_short == FALSE)

```
```{r}
nrow(dat4)


df_length = nrow(dat3) - nrow(dat4)
df_length
```






```{r tokenizing}
# unnest to word-level tokens remove stop words, and join sentiment words
text_words <- dat3  %>%
  unnest_tokens(output = word, input = Text, token = 'words')

# inspect the list of tokens (words)
# text_words$word
```




```{r text_cleaning}
clean_tokens <- str_remove_all(text_words$word, "_[a-z,A-Z]*")

clean_tokens <- str_remove_all(clean_tokens, "[:digit:]")
clean_tokens <- str_remove_all(clean_tokens, "www.[a-z,A-Z]*")

clean_tokens <- str_remove_all(clean_tokens, "(df3$z1))>=3)")
```






# We need to unnest the text to the word level so we can label the individual sentiment words. Let's also remove stop words as standard text cleaning procedure. Note: Not every English word is in the lexicons because many English words are pretty neutral.

```{r}


# break text into individual words (tokenized)
sent_words <- text_words %>%
  
  # returns only the rows without stop words
  anti_join(stop_words, by = 'word') %>%
  
  # joins and retains only sentiment words
  inner_join(get_sentiments("nrc"), by = 'word') %>% 
  
  # fitler out positive & negative
  filter(!sentiment %in% c('positive', 'negative'))
```



```{r}
sent_pct <- sent_words %>% 
  group_by(Date, sentiment) %>% 
  count(sentiment) %>%
  ungroup() %>% 
  group_by(Date) %>% 
  mutate(n_max_day = sum(n),
         percent = round((n/n_max_day)*100, 2))

```


```{r}
ggplot(data = sent_pct) +
  geom_line(aes(x = Date, y = percent, color = sentiment)) +
  labs(title = "Percent of Emotion from",
       subtitle = "term = 'inaturalist'",
       caption = "lexicon 'emotion' from Nexis-Uni") +
  theme_minimal()
```









data above before chunk below
```{r}
# grab text sentences from each paragraph
mytext <- get_sentences(dat3$Text)

sent <- sentiment(mytext)

sent_df <- inner_join(dat3, sent, by = "element_id")

sentiment <- sentiment_by(sent_df$Text)

sent_df %>%
  arrange(sentiment)

sent_df$polarity <- ifelse(test = sent_df$sentiment <0, -1, ifelse(sent_df$sentiment > 0, 1, 0))
```


```{r}



```

